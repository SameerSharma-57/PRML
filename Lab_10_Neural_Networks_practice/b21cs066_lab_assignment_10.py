# -*- coding: utf-8 -*-
"""B21CS066_Lab_Assignment_10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IKYH0r66Fgs5XUGPlUXre_0rUw8GmKYT
"""



"""#Importing libraries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torchvision
import torchvision.datasets as datasets
from sklearn.model_selection import train_test_split as tts
import torchvision.transforms as transforms
from torch.utils.data import random_split
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import  train_test_split as tts
import seaborn as sns
from sklearn.preprocessing import StandardScaler

"""#Part 1"""

columns=['Sex','Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Rings']
df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data',names=columns)
df

df.isnull().sum()

sns.pairplot(data=df,hue='Rings',palette='viridis')

df['Sex']=df['Sex'].replace(['M','F','I'],[0,1,2]).astype(int)
df

df['Rings'].value_counts()

class_no=[]
for n_rings in df['Rings']:
    if(n_rings<8):
        class_no.append(0)
    elif(n_rings<13):
        class_no.append(1)
    elif(n_rings<18):
        class_no.append(2)
    else:
        class_no.append(3)

df['Class']=class_no
df

df['Class'].value_counts()



X=df.drop(['Rings','Class'],axis=1).to_numpy()
y=df['Class'].to_numpy()
#split is going to be done in 50:20:30 ratio

std_sc=StandardScaler()
X=std_sc.fit_transform(X)

def startified_split(X,y,test_size):
    X_train=[]
    y_train=[]
    X_test=[]
    y_test=[]

    for label in np.unique(y):
        idx=np.where(y==label)[0]
        np.random.shuffle(idx)
        split=(int)(len(idx)*test_size)
        

        train_idx=idx[split:]
        test_idx=idx[:split]
        if(split==0):
            X_train.extend(X[idx])
            y_train.extend(y[idx])
            X_test.extend(X[idx])
            y_test.extend(y[idx])
        else:
            X_train.extend(X[train_idx])
            y_train.extend(y[train_idx])
            X_test.extend(X[test_idx])
            y_test.extend(y[test_idx])
    return np.array(X_train),np.array(X_test),np.array(y_train),np.array(y_test)


X_temp,X_test,y_temp,y_test=startified_split(X,y,0.3)
X_train,X_valid,y_train,y_valid=startified_split(X_temp,y_temp,(0.2/0.7))
print(X_train.shape,X_test.shape,y_train.shape,y_test.shape,X_valid.shape,y_valid.shape)

print()

print('Classes present in training data along with their counts')
unique, counts = np.unique(y_train, return_counts=True)

print(np.asarray((unique, counts)).T)

print('Classes present in testing data along with their counts')
unique, counts = np.unique(y_test, return_counts=True)
print(np.asarray((unique, counts)).T)

print('Classes present in validation data along with their counts')
unique, counts = np.unique(y_valid, return_counts=True)
print(np.asarray((unique, counts)).T)



"""##Part 2"""

def numpy_to_dataset_loader(X,y):
    
    tensor_x = torch.Tensor(X) # transform to torch tensor
    tensor_y = torch.Tensor(y)
    tensor_y=tensor_y.long()

    my_dataset = torch.utils.data.TensorDataset(tensor_x,tensor_y) # create your datset
    my_dataloader = torch.utils.data.DataLoader(my_dataset,batch_size=64,shuffle=False)
    return my_dataloader

train_loader = numpy_to_dataset_loader(X_train,y_train)
val_loader = numpy_to_dataset_loader(X_valid,y_valid)
test_loader = numpy_to_dataset_loader(X_test,y_test)



"""##Part 3"""

class MLP(nn.Module):
    def __init__(self,input_size,hidden_size_1,hidden_size_2,output_size):
        super(MLP,self).__init__()
        self.w1=nn.Linear(input_size,hidden_size_1)
        self.w2=nn.Linear(input_size,hidden_size_2)
        self.w3=nn.Linear(hidden_size_1,output_size)
        self.w4=nn.Linear(hidden_size_2,output_size)

    def forward(self,x):
        x1=self.w1(x)
        x1=nn.functional.sigmoid(x1)
        x2=self.w2(x)
        x2=nn.functional.sigmoid(x2)
        x1=self.w3(x1)
        x1=nn.functional.sigmoid(x1)
        x2=self.w4(x2)
        x2=nn.functional.sigmoid(x2)
        x=x1+x2
        return x

mlp=MLP(8,256,256,4)
no_of_trainable_parameters=0
for p in mlp.parameters():
    if(p.requires_grad):
        no_of_trainable_parameters+=p.numel()

print('No of trainable parameters are ',no_of_trainable_parameters)



"""##Part 4"""

num_epochs=50

optimizer=optim.Adam(mlp.parameters())
loss_func=nn.CrossEntropyLoss()

train_accs=[]
train_losses=[]
val_accs=[]
val_losses=[]


best_val_acc=0
best_model=mlp

for epoch in range(num_epochs):
    mlp.train()
    train_acc=0
    train_loss=0
    for inputs,labels in train_loader:
        optimizer.zero_grad()
        # inputs=inputs.reshape(-1,28*28)
        outputs=mlp(inputs)
        
        loss=loss_func(outputs,labels)
        
        loss.backward()
        optimizer.step()

        train_loss+=loss.item()*inputs.size(0)
        _,predicted=torch.max(outputs.data,1)
        train_acc+=(predicted==labels).sum().item()
    train_loss=train_loss/len(X_train)
    train_acc=train_acc/len(X_train)

    mlp.eval()

    val_loss=0
    val_acc=0

    with torch.no_grad():
        for inputs,labels in val_loader:
            # inputs=inputs.reshape(-1,28*28)
            outputs=mlp(inputs)
            loss=loss_func(outputs,labels)


            val_loss+=loss.item()*inputs.size(0)
            _,predicted=torch.max(outputs.data,1)
            val_acc+=(predicted==labels).sum().item()

            



                


    val_loss=val_loss/len(X_valid)
    val_acc=val_acc/len(X_valid)
    if(best_val_acc<val_acc):
        best_val_acc=val_acc
        best_model=mlp

    train_losses.append(train_loss)
    train_accs.append(train_acc)
    val_losses.append(val_loss)
    val_accs.append(val_acc)

    if (epoch%10==0):

        print(f'Epoch {epoch+1}/{num_epochs}: train_loss: {train_loss}, train_accuracy: {train_acc}, val_loss: {val_loss}, val_accuracy: {val_acc}')

mlp=best_model
mlp.eval()

test_loss=0
test_acc=0

with torch.no_grad():
    for inputs,labels in test_loader:
        # inputs=inputs.reshape(-1,28*28)
        outputs=mlp(inputs)
        loss=loss_func(outputs,labels)


        test_loss+=loss.item()*inputs.size(0)
        _,predicted=torch.max(outputs.data,1)
        test_acc+=(predicted==labels).sum().item()

        



            


test_loss=test_loss/len(X_test)
test_acc=test_acc/len(X_test)
print(f'test_loss: {test_loss}, test_accuracy: {test_acc}')

for title,train,val in zip(['Loss-epoch','Train-epoch'],[train_losses,train_accs],[val_losses,val_accs]):
    plt.plot(train,label='Training')
    plt.plot(val,label='Validation')
    plt.title(title)
    plt.legend()
    plt.show()

