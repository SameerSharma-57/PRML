# -*- coding: utf-8 -*-
"""B21CS066_Lab_Assignment_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wMLJlD8p8Gvh5SMvrwEmIKYzN8kvnPwQ

#Importing libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt



#sklearn imports
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split as tts
from sklearn.svm import SVC
# from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score as CVS
from sklearn.metrics import accuracy_score,f1_score 
from sklearn.metrics import confusion_matrix
from sklearn.ensemble import BaggingClassifier
from sklearn.neighbors import KNeighborsClassifier

"""#Problem 1

##Part 1
"""

columns=['family',
    'product-type',
    'steel',
    'carbon',
    'hardness',
    'temper_rolling',
    'condition',
    'formability',
    'strength',
   'non-ageing',
   'surface-finish',
   'surface-quality',
   'enamelability',
   'bc',
   'bf',
   'bt',
   'bw/me',
   'bl',
   'm',
   'chrom',
   'phos',
   'cbond',
   'marvi',
   'exptl',
   'ferro',
   'corr',
   'blue/bright/varn/clean',
   'lustre',
   'jurofm',
   's',
   'p',
   'shape',
   'thick',
   'width',
   'len',
   'oil',
   'bore',
   'packing',
   'classes']
df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/annealing/anneal.data',names=columns)
test_df=pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/annealing/anneal.test',names=columns)
df

df.describe()

df.info()

df.replace(to_replace='?',value=None,inplace=True)
test_df.replace(to_replace='?',value=None,inplace=True)

df

df.info()

test_df.info()

"""##Part 2"""

for header in df.columns:
    if(df[header].isnull().sum()>0.25*len(df[header])):
        df.drop(header,axis=1,inplace=True)
        test_df.drop(header,axis=1,inplace=True)
        

df.info()

test_df.info()

df.dropna(inplace=True)
test_df.dropna(inplace=True)
df.info()

test_df.info()

product_type_values=['C', 'H', 'G']
df['product-type'].replace(to_replace=product_type_values,value=np.arange(len(product_type_values)),inplace=True)
test_df['product-type'].replace(to_replace=product_type_values,value=np.arange(len(product_type_values)),inplace=True)

steel_values=['-','R','A','U','K','M','S','W','V']
df['steel'].replace(to_replace=steel_values,value=np.arange(len(steel_values)),inplace=True)
test_df['steel'].replace(to_replace=steel_values,value=np.arange(len(steel_values)),inplace=True)

shape_values=['COIL', 'SHEET']
df['shape'].replace(to_replace=shape_values,value=np.arange(len(shape_values)),inplace=True)
test_df['shape'].replace(to_replace=shape_values,value=np.arange(len(shape_values)),inplace=True)

df['classes'].replace(to_replace='U',value=6,inplace=True)
test_df['classes'].replace(to_replace='U',value=6,inplace=True)

df

test_df

X=df.drop(['classes','product-type'],axis=1)
y=df['classes'].to_numpy()

X_test=test_df.drop('classes',axis=1)
y_test=test_df['classes'].to_numpy()

std_sc=StandardScaler()
std_sc.fit(X)
std_X=std_sc.transform(X)
std_sc.fit(X_test)
std_X_test=std_sc.transform(X_test)
pd.DataFrame(std_X)

X=X.to_numpy().astype(float)
X_test=X_test.to_numpy().astype(float)

X_train,X_valid,y_train,y_valid=tts(X,y,test_size=0.35)
std_X_train,std_X_valid,std_y_train,std_y_valid=tts(X,y,test_size=0.35)

print(X_train.shape,X_valid.shape,std_X_train.shape,std_X_valid.shape)

X

"""##Part 3"""

y=np.array(y,dtype=float)
X_train=np.array(X_train,dtype=float)
X_valid=np.array(X_valid,dtype=float)
y_train=np.array(y_train,dtype=float)
y_valid=np.array(y_valid,dtype=float)
std_X_train=np.array(std_X_train,dtype=float)
std_X_valid=np.array(std_X_valid,dtype=float)
std_y_train=np.array(std_y_train,dtype=float)
std_y_valid=np.array(std_y_valid,dtype=float)
std_X=np.array(std_X,dtype=float)
X=np.array(X,dtype=float)

clf_svc=SVC().fit(std_X_train,std_y_train)
clf_rfc=RandomForestClassifier().fit(std_X_train,std_y_train)
clf_dtc=DecisionTreeClassifier().fit(std_X_train,std_y_train)

clf_svc.score(std_X_valid,std_y_valid)

plt.rcParams["figure.figsize"] = [7.50, 3.50]

for clf in [clf_svc,clf_rfc,clf_dtc]:
    scores=CVS(clf,std_X,y,cv=5)
    plt.subplot(1,2,1)
    plt.plot(scores,label=str(clf),marker='o')
    plt.title("For standardized data")
    


for clf in [clf_svc,clf_rfc,clf_dtc]:
    scores=CVS(clf,X,y,cv=5)
    plt.subplot(1,2,2)
    plt.plot(scores,label=str(clf),marker='o')
    plt.title("For non standardized data")
    
plt.legend(bbox_to_anchor=(1.1, 1.05))
plt.show()



"""##Part4"""

# function to compute mean of a 1D array
def mean_scratch(X):
    n=len(X)
    sm=0.0
    for x in X:
        sm+=x
    return sm/n

# function to compute covariance of two columns X and Y
def covariance_scratch(X,Y):
  mean_x=np.mean(X) #x_bar
  mean_y=np.mean(Y) #y_bar
  temp_X=X-mean_x #X-x_bar
  temp_Y=Y-mean_y #Y-y_bar
  temp=np.multiply(temp_X,temp_Y) #(X-x_bar).(Y-y_bar)
  return sum(temp)/(len(temp)-1) #sum((x_i=x_bar)*(y_i-y_bar))/(n-1)


#function to compute covariance matrix of a dataset df
def cov_matrix(df):
    
    cov_mat=np.zeros((len(df.columns),len(df.columns))) #initializing the covariance matrix with zeros
    for i in range(len(df.columns)):
        for j in range(len(df.columns)):
            cov_mat[i][j]=covariance_scratch(df[df.columns[i]].to_numpy(),df[df.columns[j]].to_numpy()) #updating cov_mat[i][j] to the covariance of columns at ith and jth index in the dataset
    return cov_mat


#function to centralize a numpy array (df-mean(df))/std(df) Thus it makes mean of all columns zero and standard deviation one.
def centralize(Y):
    X=Y.copy()
    n=len(X)
    for i in range(X.shape[1]):
        x=X[:,i] # slicing out ith column
        x-=mean_scratch(x) # x-x_bar
        std=(covariance_scratch(x,x))**0.5 #computing standard deviation for the column x
        if(std==0):
            continue #if standard deviation is zero, it means that column contains a single value that is zero. In this case, there is no need to do divide it with std
        x=x/std #(x-x_bar)/std(x)
        X[:,i]=x #updating the ith column in the dataset to the centralized data 
    return X

print("Covariance Matrix for centralized data")
scratch_std_X=pd.DataFrame(centralize(X_train),columns=df.columns[1:-1])
centralized_cov=cov_matrix(scratch_std_X)
show=pd.DataFrame(centralized_cov,columns=df.columns[1:-1])
show.set_index(df.columns[1:-1],inplace=True)

show

print("Covariance Matrix for uncentralized data")
show_un=pd.DataFrame(cov_matrix(pd.DataFrame(X_train,columns=df.columns[1:-1])),columns=df.columns[1:-1])
show_un.set_index(df.columns[1:-1],inplace=True)
show_un

# function to compute the sorted eigen vectors of a covariance matrix of the dataset
def principalComponents(mat):
    eigvalues,eigvectors=np.linalg.eig(mat) #computing eigen values and corressponding eigen vectors for the covariance matrix
    idx=np.argsort(eigvalues)[::-1] # sorting the indices in decreasing order of eigenvalues
    return eigvalues[idx],eigvectors[idx] #returning sorted eigen values and eigen vectors

principalComponents(centralized_cov)

#function to reduce the dataset by using principal component analysis (PCA)
#here n is the number of features in the reduced data
def pca(X,n):
    x=centralize(X) #centralizing the data
    cov_mat=cov_matrix(pd.DataFrame(x,columns=np.arange(x.shape[1]))) #computing covariance matrix of the centralized data
    _,pc=principalComponents(cov_mat) #getting array of sorted eigen vectors of the covariance matrix
    pc=pc[:n] #picking top n eigen vectors with higher eigen values
    X_reduced=np.dot(pc,x.T).T #reducing the data to n features by some simple matrix operations
    return X_reduced

pd.DataFrame(pca(X_train,2),columns=np.arange(2))

"""## Part 5

"""

eigenvalues,eigenvectors=principalComponents(centralized_cov)

explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)

plt.bar(np.arange(len(eigenvalues))+1,explained_variance_ratio,label="individual variance ratio")
plt.step(np.arange(len(eigenvalues))+1,cumulative_explained_variance_ratio,label="cumulative variance ratio")
plt.xlabel('n_components')
plt.ylabel('variance_ratio')
plt.legend(bbox_to_anchor=(1.1, 1.05))

plt.rcParams["figure.figsize"] = [10, 10]
for n_components in range(2,10):
    X_pca=pca(X_train,n_components)
    W=eigenvectors[:n_components]
    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)
    # plt.legend(bbox_to_anchor=(1.1, 1.05))
    for i in range(n_components):
        plt.arrow(0, 0, W[i, 0], W[i, 1], alpha=0.5, width=0.1, head_width=0.25, head_length=0.35, color='r')
    plt.xlabel('PC1')
    plt.ylabel('PC2')
    plt.show()



"""##Part 6"""

X_reduced=pca(X,2)
X_reduced_train,X_reduced_valid,y_reduced_train,y_reduced_valid=tts(X_reduced,y,test_size=0.3)
print(X_reduced_train.shape,X_reduced_valid.shape)

clf_svc=SVC()
clf_rfc=RandomForestClassifier()
clf_dtc=DecisionTreeClassifier()

plt.rcParams["figure.figsize"] = [7.50, 3.50]

for clf in [clf_svc,clf_rfc,clf_dtc]:
    scores=CVS(clf,X_reduced,y,cv=5)
    # plt.subplot(1,2,1)
    plt.plot(scores,label=str(clf),marker='o')
plt.title("Cross Validation plot on reduced dataset")
plt.xlabel('no of iteration')
plt.ylabel('accuracy')    



    
plt.legend(bbox_to_anchor=(1.5, 1.05))
plt.show()



"""##Part 7"""

clf_svc=SVC().fit(X_train,y_train)
clf_rfc=RandomForestClassifier().fit(X_train,y_train)
clf_dtc=DecisionTreeClassifier().fit(X_train,y_train)

clf_svc_reduced=SVC().fit(X_reduced_train,y_reduced_train)
clf_rfc_reduced=RandomForestClassifier().fit(X_reduced_train,y_reduced_train)
clf_dtc_reduced=DecisionTreeClassifier().fit(X_reduced_train,y_reduced_train)

scores=[[],[]]
for clf in [clf_svc,clf_rfc,clf_dtc]:
    
    y_pred=clf.predict(X_valid)
    scores[0].append(accuracy_score(y_valid,y_pred))
for clf in [clf_svc_reduced,clf_rfc_reduced,clf_dtc_reduced]:
    
    y_pred=clf.predict(X_reduced_valid)
    scores[1].append(accuracy_score(y_reduced_valid,y_pred))
print("accuracy score")
show=pd.DataFrame(scores,columns=['SVM','Random Forest Classifier','Decision Tree Classifier'],index=['Unreduced','reduced'])
# rows=['Unreduced','reduced']
# show.set_index(rows,inplace=True)
# show.set_index(['Unreduced','reduced'],inplace=True)
show

scores=[[],[]]
for clf in [clf_svc,clf_rfc,clf_dtc]:
    
    y_pred=clf.predict(X_valid)
    scores[0].append(f1_score(y_valid,y_pred,average='weighted'))
for clf in [clf_svc_reduced,clf_rfc_reduced,clf_dtc_reduced]:
    
    y_pred=clf.predict(X_reduced_valid)
    scores[1].append(f1_score(y_reduced_valid,y_pred,average='weighted'))
print("f1 score")
show=pd.DataFrame(scores,columns=['SVM','Random Forest Classifier','Decision Tree Classifier'],index=['Unreduced','reduced'])
# rows=['Unreduced','reduced']
# show.set_index(rows,inplace=True)
# show.set_index(['Unreduced','reduced'],inplace=True)
show



"""##Part 8"""

from pyparsing import line
plt.rcParams["figure.figsize"] = [10, 10]
eigenvalues,eigenvectors=principalComponents(centralized_cov)

explained_variance_ratio = eigenvalues / np.sum(eigenvalues)
cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)

# plt.bar(np.arange(len(eigenvalues))+1,explained_variance_ratio,label="individual variance ratio")
# plt.step(np.arange(len(eigenvalues))+1,cumulative_explained_variance_ratio,label="cumulative variance ratio")
plt.plot(np.arange(len(eigenvalues))+1,cumulative_explained_variance_ratio,marker='o',linestyle='--')
plt.axhline(y=0.95,color='r')
plt.text(0.7, 0.96,"95% threshold line",color='r')
plt.xlabel('n_components')
plt.ylabel('variance_ratio')
plt.legend(bbox_to_anchor=(1.1, 1.05))

X_reduced=pca(X,8)
pd.DataFrame(X_reduced,columns=np.arange(8)).corr()

"""#Problem 2

##Part 1
"""

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"
headers = ['class', 'alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']
df = pd.read_csv(url,names=headers)
df

df.info()

X=df.drop('class',axis=1).to_numpy()
y=df['class'].to_numpy()
std_sc=StandardScaler()
std_sc.fit(X)
X=std_sc.transform(X)

pd.DataFrame(X,columns=df.columns[1:])



class LDA:
    #constructor to set value for threshold variance
    def __init__(self,var_thres=0.95):
        self.var_thres=var_thres

    #function to compute scatter within class(Sw) and scatter between class (Sb)
    def __get_scatter(self,X,y):
        overall_mean=np.mean(X,axis=0) #calculating mean of whole dataset
        t_overall_mean=overall_mean.reshape(-1,1)

        #initializing the Sw and Sb matrices
        Sw=np.zeros((X.shape[1],X.shape[1]))
        Sb=np.zeros((X.shape[1],X.shape[1]))

        #computing class_mean for every class
        class_means=np.array([np.mean(X[y==label],axis=0) for label in self.classes])


        for idx,class_no in enumerate(self.classes):
            
            class_mean=class_means[idx] #getting the mean idx th class
            t_class_mean=class_mean.reshape(-1,1)
            sw=np.zeros((X.shape[1],X.shape[1])) #initializing scatter within idx th class with zeros
            for x in X[y==class_no]:
                x=x.reshape(-1,1)
                sw+=(x-t_class_mean).dot((x-t_class_mean).T) #sw=sum(x_i-x_bar).((x_i-x_bar).transpose)
            Sw+=sw # adding scatter within idx th class to Sw


            n_samples=X[y==class_no].shape[0] #calculating number of datapoints in idx th class
            Sb+=n_samples*(t_class_mean-t_overall_mean).dot((t_class_mean-t_overall_mean).T) #Sb=sum(class_mean-overall_mean).((class_mean-overall_mean).transpose)

        # print('Sw',Sw)
        # print('Sb',Sb)
        self.Sw=Sw
        self.Sb=Sb
        self.class_means=np.array(class_means)


    #function to compute linear discriminants
    def __get_eigen_vectors(self):
        

        final_mat=np.dot(np.linalg.inv(self.Sw),(self.Sb)) #computing inv(Sw).Sb
        eig_val,eig_vec=np.linalg.eig(final_mat) #computing eigen values and vectors of final matrix (inv(Sw).Sb)
        eig_vec=eig_vec.astype(float)
        eig_vec=eig_vec.T
        idx=np.argsort(abs(eig_val))[::-1] #sorting indices in decreasing order of eigen values
        sorted_eig_val=eig_val[idx] #sorting eigen values
        sorted_eig_vec=eig_vec[idx] # soring eigen vectors with decreaseing eigen values
        cum_var=np.cumsum(sorted_eig_val)/np.sum(sorted_eig_val) #calculating cumulative variance

        #computing n_components for which cumulative variance exceeds threshold variance
        n_components=0
        n_vec=len(eig_vec)
        while(n_components<n_vec-1 and cum_var[n_components]<self.var_thres):
            n_components+=1;
        

        self.proj_mat=sorted_eig_vec[:n_components+1] #getting linear discriminants/projection matrix by taking top n_component+1 eigen vectors
        self.n_components=n_components+1
        self.final_mat=final_mat
        self.eig_vec=sorted_eig_vec
        
        # print(eig_val)



    def fit(self,X,y):
        self.classes=np.unique(y) #computing classes in dataset
        self.__get_scatter(X,y) #computing Sw and Sb
        self.__get_eigen_vectors() #compputing linear discriminants
        


    def transform(self,X):
        proj_mat=self.proj_mat #getting projection matrix
        X_reduced=X.dot(proj_mat.T) #projecting the data on projection matrix and reducing it
        return X_reduced

    def predict(self,X,transformed=False):
        X_red=X
        if(transformed==False):
            X_red=self.transform(X) #transforming the test data and reducing its dimension to fit with that of training data

        class_means=self.transform(self.class_means) #transforming class means
        classes=self.classes
        preds=[]
        for x in X_red:
            dis=np.array([np.linalg.norm(x-class_mean) for class_mean in class_means]) #computing distance of sample from each class's mean
            preds.append(classes[np.argmin(dis)]) # making the class with least distance from the sample as predicted class
        return np.array(preds)



    def plot_feature_space(self,X,y):
        X_red=self.transform(X) #transforming the data

        #checking if data contains only one feature then plot datapoints on a line
        if(X_red.ndim==1 or X_red.shape[1]==1):
            plot_x=X_red
            plot_y=np.zeros(len(X_red))
        else:
            plot_x=X_red[:,0]
            plot_y=X_red[:,1]

        #scattering the points classwise with different colors
        for class_no in np.unique(y):
            idx=np.where(y==class_no)[0]
            plt.scatter(plot_x[idx],plot_y[idx],label=class_no)

        plt.legend(bbox_to_anchor=(1.1, 1.05))
        plt.show()


    def predict_proba(self,X):
        X_red=self.transform(X) #transforming the data
        class_means=self.transform(self.class_means) #transforming class means
        probas=list()
        for x in X_red:
            dis=np.array([np.linalg.norm(x-class_mean) for class_mean in class_means]) #computing distance of sample from each class's mean
            dis=dis/np.sum(dis) #normalizing the distancing
            probas.append(dis) #appending the distance to the probas list
        return np.array(probas)

    
    
    def plot_decision_boundary(self,clf,X,Y):
        # print(n)
        # new_clf_RFC=  BaggingClassifier(base_estimator=SVC(),n_estimators=n ).fit(X_train,Y_train)
        # print(new_clf_RFC.score(X_test,Y_test))
        n_classes = len(self.classes)
        plot_colors = "ryb"
        plot_step = 0.04
        


        # X=self.transform(X)
            
            

        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                            np.arange(y_min, y_max, plot_step))
        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)

        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

        plt.xlabel("X1")
        plt.ylabel("X2")

        # Plot the training points
        for i, color in zip(range(n_classes), plot_colors):
            idx = np.where(Y == self.classes[i])
            plt.scatter(X[idx, 0], X[idx, 1], c=color, label=i,
                        cmap=plt.cm.RdYlBu, edgecolor='black', s=16)


        plt.legend(loc='lower right', borderpad=0, handletextpad=0)
        plt.axis("tight")


        plt.show()

lda=LDA(0.95)
lda.fit(X,y)
# pd.DataFrame(lda.transform(X),columns=np.arange(2))
# lda.predict(X)
accuracy_score(lda.predict(X),y)

X_train,X_test,y_train,y_test=tts(X,y,test_size=0.3,shuffle=True)

lda.fit(X_train,y_train)
accuracy_score(lda.predict(X_test),y_test)

"""##Part 2"""

variances=np.arange(0,1,0.01)
n_components=[]
for var in variances:
    temp_lda=LDA(var)
    temp_lda.fit(X_train,y_train)
    n_components.append(temp_lda.n_components)

plt.plot(variances,n_components)
plt.xlabel('variance_threshold')
plt.ylabel('n_components')

n_components=np.unique(n_components)
for var in [0.6,0.9]:
    temp_lda=LDA(var)
    temp_lda.fit(X,y)
    temp_lda.plot_feature_space(X,y)



"""##Part 3"""



def mean_scratch(X):
    n=len(X)
    sm=0.0
    for x in X:
        sm+=x
    return sm/n


def covariance_scratch(X,Y):
  mean_x=np.mean(X)
  mean_y=np.mean(Y)
  temp_X=X-mean_x
  temp_Y=Y-mean_y
  temp=np.multiply(temp_X,temp_Y)
  return sum(temp)/(len(temp)-1)

def cov_matrix(df):
    
    cov_mat=np.zeros((len(df.columns),len(df.columns)))
    for i in range(len(df.columns)):
        for j in range(len(df.columns)):
            cov_mat[i][j]=covariance_scratch(df[df.columns[i]].to_numpy(),df[df.columns[j]].to_numpy())
    return cov_mat


def centralize(Y):
    X=Y.copy()
    n=len(X)
    for i in range(X.shape[1]):
        x=X[:,i]
        # print(x)
        x-=mean_scratch(x)
        # print(mean_scratch(x))
        std=(covariance_scratch(x,x))**0.5
        if(std==0):
            continue
        x=x/std
        # print(x==X[i])
        X[:,i]=x
    return X

def principalComponents(mat):
    eigvalues,eigvectors=np.linalg.eig(mat)
    idx=np.argsort(eigvalues)[::-1]
    return eigvalues[idx],eigvectors[idx]




def pca(X,n):
    x=centralize(X)
    cov_mat=cov_matrix(pd.DataFrame(x,columns=np.arange(x.shape[1])))
    _,pc=principalComponents(cov_mat)
    pc=pc[:n]

    X_reduced=np.dot(pc,x.T).T

    
    return X_reduced

X_reduced_pca=pca(X,2)
X_pca_train,X_pca_test,y_pca_train,y_pca_test=tts(X_reduced_pca,y,shuffle=True,test_size=0.3)

lda=LDA()
lda.fit(X,y)
X_reduced_lda=lda.transform(X)

X_lda_train,X_lda_test,y_lda_train,y_lda_test=tts(X_reduced_lda,y,shuffle=True,test_size=0.3)

svm_clf=SVC()
DTC_clf=DecisionTreeClassifier()

scores=np.zeros((2,2))
svm_clf.fit(X_pca_train,y_pca_train)
DTC_clf.fit(X_pca_train,y_pca_train)
scores[0]=np.array([clf.score(X_pca_test,y_pca_test) for clf in [svm_clf,DTC_clf]])

svm_clf.fit(X_lda_train,y_lda_train)
DTC_clf.fit(X_lda_train,y_lda_train)
scores[1]=np.array([clf.score(X_lda_test,y_lda_test) for clf in [svm_clf,DTC_clf]])


pd.DataFrame(scores,columns=['SVM classifier','DTC classifier'],index=['pca','lda'])



"""##Part 4"""

pd.DataFrame(scores,columns=['SVM classifier','DTC classifier'],index=['pca','lda'])

print('Decision boundary for svm classifier')
lda.plot_decision_boundary(svm_clf,X_lda_test,y_lda_test)

print('Decision boundary for Decision Tree classifier')
lda.plot_decision_boundary(DTC_clf,X_lda_test,y_lda_test)

lda.plot_feature_space(X,y)



"""##Part 5"""

lda_clf=LDA()
lda_clf.fit(X_train,y_train)
lda_clf.predict_proba(X_test)

def plot_roc_curve(clf,X,y):
    y_probas=clf.predict_proba(X)
    auc_s=[]
    for idx,class_no in enumerate(clf.classes):
        y_true=np.where(y==class_no,1,0)
        y_probs=y_probas[:,idx]
        thresholds=np.sort(y_probs)[::-1]
        tpr=[]
        fpr=[]
        for thres in thresholds:
            y_pred=np.where(y_probs>=thres,1,0)
            tn,fp,fn,tp=confusion_matrix(y_true,y_pred).ravel()
            tpr.append(tp/(tp+fn))
            fpr.append(fp/(fp+tn))
        auc=np.trapz(fpr,tpr)
        auc_s.append(auc)
        plt.plot(tpr,fpr,label=class_no,alpha=0.5,lw=5)
    plt.xlabel('True Positive Rate')
    plt.ylabel('False Positive Rate')
    plt.legend(bbox_to_anchor=(1.1, 1.05))
    plt.show()
    print("AUC: ",auc_s)

plot_roc_curve(lda_clf,X_train,y_train)

lda_clf=LDA()

def cross_validation(clf,k,X,y):
    indices = np.random.permutation(X.shape[0])
    fold_size = X.shape[0] // k
    validation_scores = np.zeros(k)
    for i in range(k):
        val_indices = indices[i*fold_size:(i+1)*fold_size]
        train_indices = np.concatenate((indices[:i*fold_size], indices[(i+1)*fold_size:]))
        X_train, y_train = X[train_indices], y[train_indices]
        X_val, y_val = X[val_indices], y[val_indices]
        clf.fit(X_train, y_train)
        score = accuracy_score(clf.predict(X_val),y_val)
        validation_scores[i] = score
        plot_roc_curve(clf,X_train,y_train)
    return validation_scores


plt.plot(cross_validation(lda_clf,5,X,y),marker='o')

