# -*- coding: utf-8 -*-
"""B21CS066_Lab_Assignment_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12iwlH31my5ZZ7vQ6UZVhDtTpf6i3bEw-

#Lab_02

##importing libraries
"""

from operator import imod
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
from sklearn.preprocessing import normalize
from sklearn.model_selection import train_test_split as tts
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error as mse
from sklearn.metrics import accuracy_score as acc
from sklearn.model_selection import KFold
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import cross_val_score
from sklearn import tree
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from math import sin,cos,pi

"""##Problem 01

###Part 1

####Loading dataset
"""

df=pd.read_csv("/content/drive/MyDrive/PRML/Lab-02/ENB2012_data.xlsx - Φύλλο1.csv")
df

"""####handling null values"""

df=df.dropna()
df

"""####encoding the data

since all data is allready in interval and ratio format there is no need to encode it

####reducing noise
"""

for dataheader in df.columns:
  for j in range(df[dataheader].size):
    try:
      df[dataheader][j]=df[dataheader][j].astype(float)
    except:
      df[dataheader][j]="0"
df

"""####normalize the data

"""

Y=df["Y1"].to_numpy()
df=df.drop("Y1",axis=1)
for dataheaders in df.columns:
  df[dataheaders]=normalize([df[dataheaders]])[0]
print(df)

"""####spliting the data into training,validation,testing"""

X=df.to_numpy()

X_train,X_temp,Y_train,Y_temp=tts(X,Y,test_size=0.3,shuffle=True,random_state=42)
X_test,X_valid,Y_test,Y_valid=tts(X_temp,Y_temp,test_size=1/3,shuffle=True,random_state=42)
print(X_train.shape,X_valid.shape,X_test.shape)

"""###Part 2

"""

reg=DecisionTreeRegressor(criterion="squared_error",splitter="best",max_depth=7,max_features=2,min_impurity_decrease=0.0,min_samples_split=12);
reg.fit(X_train,Y_train)

y_pred_valid=reg.predict(X_valid)
print("Score of the decision tree is ",reg.score(X_valid,Y_valid))
print("mse of the decision tree is ",mse(y_pred_valid,Y_valid))


# plt.rcParams['figure.figsize'] = [100, 10]

x=range(Y_valid.size)
plt.scatter(x,Y_valid,color="red")
plt.plot(x,y_pred_valid)
plt.show()

max_depth = np.arange(2,100,1)
print(max_depth.shape)
y_max_depth = np.zeros(max_depth.shape)
print(y_max_depth.shape)
min = 1000
min_idx=-1
for i in max_depth:
  # print("for max depth = ",i)
  temp_reg=DecisionTreeRegressor(max_depth=i);
  temp_reg.fit(X_train,Y_train)

  y_pred_valid=temp_reg.predict(X_valid)
  # print("Score of the decision tree is ",temp_reg.score(X_valid,Y_valid))
  # print("mse of the decision tree is ",mse(y_pred_valid,Y_valid))
  y_max_depth[i-2] = mse(y_pred_valid,Y_valid)
  if(y_max_depth[i-2]<min):
      min=y_max_depth[i-2]
      min_idx = i;
  


  # plt.rcParams['figure.figsize'] = [100, 10]

plt.plot(max_depth,y_max_depth)
plt.xlabel("max_depth")
plt.ylabel("mse")
plt.show()
print(min_idx)

min_sample_splits = np.arange(2,50,1)
y_min_sample_splits = np.zeros(min_sample_splits.shape)
min = 1000
min_idx=-1
for i in min_sample_splits:
  # print("for min_sample_split = ",i)
  temp_reg=DecisionTreeRegressor(min_samples_split=i);
  temp_reg.fit(X_train,Y_train)

  y_pred_valid=temp_reg.predict(X_valid)
  y_min_sample_splits[i-2] = mse(y_pred_valid,Y_valid) 
  if(y_min_sample_splits[i-2]<min):
      min=y_min_sample_splits[i-2]
      min_idx=i
  

  # plt.rcParams['figure.figsize'] = [100, 10]

plt.plot(min_sample_splits,y_min_sample_splits)
plt.xlabel("min_sample_splits")
plt.ylabel("mse")
plt.show()
print(min_idx)

min_impurity_decrease = np.linspace(0,1,1000)
y_min_impurity_decrease = np.zeros(min_impurity_decrease.shape)
min = 1000
min_idx=-1
idx=0
for i in min_impurity_decrease:
  # print("for min_sample_split = ",i)
  temp_reg=DecisionTreeRegressor(min_impurity_decrease=i);
  temp_reg.fit(X_train,Y_train)

  y_pred_valid=temp_reg.predict(X_valid)
  y_min_impurity_decrease[idx] = mse(y_pred_valid,Y_valid) 
  if(y_min_impurity_decrease[idx]<min):
      min=y_min_impurity_decrease[idx]
      min_idx=i
  idx+=1
  

  # plt.rcParams['figure.figsize'] = [100, 10]

plt.plot(min_impurity_decrease,y_min_impurity_decrease)
plt.xlabel("min_impurity_decrease")
plt.ylabel("mse")
plt.show()
print(min_idx)

"""###Part 3

####Hold out cross validation
"""

temp_reg=DecisionTreeRegressor(criterion="squared_error",splitter="best",max_depth=8,max_features=2,min_impurity_decrease=0.001,min_samples_split=4);
x_train,x_test,y_train,y_test = tts(X,Y,test_size=0.3,random_state=42,shuffle=True)
temp_reg.fit(x_train,y_train)
y_pred=temp_reg.predict(x_test)
print("Score of decision tree on hold out validation for test_size = 0.3 is ",temp_reg.score(x_test,y_test))

x=range(y_test.size)
plt.scatter(x,y_pred)
plt.plot(x,y_test)
plt.show()

"""####5-fold cross validation"""

temp_reg=DecisionTreeRegressor(criterion="squared_error",splitter="best",max_depth=8,max_features=2,min_impurity_decrease=0.001,min_samples_split=4);
cv=KFold(n_splits=5,random_state=42,shuffle=True)
scores = cross_val_score(temp_reg, X, Y,cv=cv,n_jobs=-1)
print("mean score ",np.mean(scores))
print("variance ",np.var(scores))

"""####repeated 5-fold validation

"""

temp_reg=DecisionTreeRegressor(criterion="squared_error",splitter="best",max_depth=8,max_features=2,min_impurity_decrease=0.001,min_samples_split=4);
cv=RepeatedKFold(n_splits=5,n_repeats=10,random_state=42)
scores = cross_val_score(temp_reg, X, Y,cv=cv,n_jobs=-1)
print("mean score ",np.mean(scores))
print("variance ",np.var(scores))

"""####mse calculation for test data"""

y_pred_test=reg.predict(X_test)
print("Score of the decision tree is ",reg.score(X_test,Y_test))
print("mse of the decision tree is ",mse(y_pred_test,Y_test))

"""####Plotting the decision tree

"""

tree.plot_tree(reg)

"""###Part 4"""

# L2 loss reduction
X_train_new=X_train[:,:1]
X_test_new=X_test[:,:1]
temp_model = DecisionTreeRegressor(criterion="squared_error",max_depth=2).fit(X_train_new,Y_train)


X_grid=np.arange(min(X_test_new),max(X_test_new),0.000001)

X_grid=X_grid.reshape(len(X_grid),1)
Y_pred=temp_model.predict(X_grid)
print(temp_model.score(X_test_new.reshape(-1,1),Y_test))

plt.plot(X_grid,Y_pred,color="b")
plt.scatter(X_test_new,Y_test,color="r")
plt.xlabel("X0")
plt.ylabel("Y")
plt.show()

#L1 loss reduction
X_train_new=X_train[:,:1]
X_test_new=X_test[:,:1]
temp_model = DecisionTreeRegressor(criterion="absolute_error",max_depth=2).fit(X_train_new,Y_train)


X_grid=np.arange(min(X_test_new),max(X_test_new),0.000001)

X_grid=X_grid.reshape(len(X_grid),1)
Y_pred=temp_model.predict(X_grid)
print(temp_model.score(X_test_new.reshape(-1,1),Y_test))

plt.plot(X_grid,Y_pred,color="b")
plt.scatter(X_test_new,Y_test,color="r")
plt.xlabel("X0")
plt.ylabel("Y")
plt.show()

"""##Problem 2

###Classification

####Part 1

Loading the iris dataset
"""

cols = ['sepal_length', ' sepal_width', 'petal_length', 'petal_width', 'class']
df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",names=cols)
df

"""encoding the dataset"""

lab_enc = LabelEncoder()
df["class"] = lab_enc.fit_transform(df["class"])
df

"""Spliting the dataset"""

X=df[["petal_length","petal_width"]]
Y=df["class"]
X_train,X_test,Y_train,Y_test = tts(X,Y,test_size=0.2,shuffle=True,random_state=42)
# print(X_train.shape,X_test.shape)
print(X)

"""Training the Dataset"""

model = DecisionTreeClassifier(max_depth=2)
model.fit(X_train,Y_train)
print(model.score(X_test,Y_test))
tree.plot_tree(model)

n_classes = 3
plot_colors = "ryb"
plot_step = 0.02
cols = ["Iris-setosa","Iris-versicolor","Iris-virginica"]

x_min, x_max = X["petal_length"].min() - 1, X["petal_length"].max() + 1
y_min, y_max = X["petal_width"].min() - 1, X["petal_width"].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),np.arange(y_min, y_max, plot_step))
plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)

Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

plt.xlabel("petal_length")
plt.ylabel("petal_width")

# Plot the training points
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(Y == i)
    
    plt.scatter(X["petal_length"][idx[0]], X["petal_width"][idx[0]],label = cols[i], c=color,cmap=plt.cm.RdYlBu, edgecolor='black', s=15)

plt.suptitle("Decision boundary of a decision tree")
plt.legend(loc='lower right', borderpad=0, handletextpad=0)
plt.axis("tight")


plt.show()

"""####Part 2

Removing the widest iris-versicolor
"""

new_df=df.copy()
print(df["class"][70])
for i in range(len(Y)):
  if(X["petal_length"][i]==4.8 and X["petal_width"][i]==1.8 and Y[i]==1):
    print(i)
    new_df.drop(i,inplace=True)
    break;
new_df.set_index(np.arange(0,new_df.shape[0]),inplace=True)
new_X=new_df[["petal_length","petal_width"]]
new_Y=new_df[["class"]]
X_train_new,X_test_new,Y_train_new,Y_test_new=tts(new_X,new_Y,test_size=80,shuffle=True)

"""Training a decision tree"""

new_model=DecisionTreeClassifier()
new_model.fit(X_train_new,Y_train_new)
n_classes = 3
plot_colors = "ryb"
plot_step = 0.02
cols = ["Iris-setosa","Iris-versicolor","Iris-virginica"]

x_min, x_max = new_X["petal_length"].min() - 1, new_X["petal_length"].max() + 1
y_min, y_max = new_X["petal_width"].min() - 1, new_X["petal_width"].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),np.arange(y_min, y_max, plot_step))
plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)

Z = new_model.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

plt.xlabel("petal_length")
plt.ylabel("petal_width")

# Plot the training points
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(new_Y == i)
    
    plt.scatter(new_X["petal_length"][idx[0]], new_X["petal_width"][idx[0]],label = cols[i], c=color,cmap=plt.cm.RdYlBu, edgecolor='black', s=15)

plt.suptitle("Decision boundary of a decision tree")
plt.legend(loc='lower right', borderpad=0, handletextpad=0)
plt.axis("tight")


plt.show()

"""####Part 3"""

model_with_max_depth_none = DecisionTreeClassifier(max_depth=None)
model_with_max_depth_none.fit(X_train,Y_train)
print(model_with_max_depth_none.score(X_test,Y_test))

n_classes = 3
plot_colors = "ryb"
plot_step = 0.02
cols = ["Iris-setosa","Iris-versicolor","Iris-virginica"]

x_min, x_max = X["petal_length"].min() - 1, X["petal_length"].max() + 1
y_min, y_max = X["petal_width"].min() - 1, X["petal_width"].max() + 1

xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),np.arange(y_min, y_max, plot_step))
plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)

Z = model_with_max_depth_none.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

plt.xlabel("petal_length")
plt.ylabel("petal_width")

# Plot the training points
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(Y == i)
    
    plt.scatter(X["petal_length"][idx[0]], X["petal_width"][idx[0]],label = cols[i], c=color,cmap=plt.cm.RdYlBu, edgecolor='black', s=15)

plt.suptitle("Decision boundary of a decision tree")
plt.legend(loc='lower right', borderpad=0, handletextpad=0)
plt.axis("tight")


plt.show()

"""####Part 4"""

temp_x=np.zeros((200,3))

temp_x[:,0]=np.random.default_rng().uniform(low=0,high=5,size=(200,))
temp_x[:,1]=np.random.default_rng().uniform(low=0,high=5,size=(200,))



for i in range(temp_x.shape[0]):
  if temp_x[i,0]>2.5:
    temp_x[i,2]=0
  else:
    temp_x[i,2]=1
idx=np.where(temp_x[:,2]==1)
print(len(idx[0]))

new_df=pd.DataFrame(data=temp_x,index=np.arange(0,200),columns=["X1","X2","Y"])
new_df

X=new_df.iloc[:,:2].to_numpy()
Y=new_df.iloc[:,2:].to_numpy()
X_train,X_test,Y_train,Y_test = tts(X,Y,test_size=0.2,shuffle=True)
print(X_train.shape,X_test.shape)

new_clf = DecisionTreeClassifier(max_depth=2).fit(X_train,Y_train)
n_classes = 2
plot_colors = "rb"
plot_step = 0.02




    
    

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                     np.arange(y_min, y_max, plot_step))
plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)

Z = new_clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

plt.xlabel("X1")
plt.ylabel("X2")

# Plot the training points
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(Y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=i,
                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)


plt.legend(loc='lower right', borderpad=0, handletextpad=0)
plt.axis("tight")


plt.show()

temp=temp_x.copy()
thetha=(-1)*pi/4
temp[:,0]=temp[:,0]*cos(thetha)-temp[:,1]*sin(thetha)
temp[:,1]=temp[:,0]*sin(thetha)+temp[:,1]*cos(thetha)
idx=np.where(temp[:,2]==1)
plt.scatter(temp[idx,0],temp[idx,1],color="r",cmap=plt.cm.RdYlBu)
idx=np.where(temp[:,2]==0)
plt.scatter(temp[idx,0],temp[idx,1],color="b",cmap=plt.cm.RdYlBu)

new_df=pd.DataFrame(data=temp,index=np.arange(0,200),columns=["X1","X2","Y"])
new_df

X=new_df.iloc[:,:2].to_numpy()
Y=new_df.iloc[:,2:].to_numpy()
X_train,X_test,Y_train,Y_test = tts(X,Y,test_size=0.2,shuffle=True)
print(X_train.shape,X_test.shape)

new_clf = DecisionTreeClassifier(max_depth=2).fit(X_train,Y_train)
n_classes = 2
plot_colors = "rb"
plot_step = 0.02




    
    

x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),
                     np.arange(y_min, y_max, plot_step))
plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)

Z = new_clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)

plt.xlabel("X1")
plt.ylabel("X2")

# Plot the training points
for i, color in zip(range(n_classes), plot_colors):
    idx = np.where(Y == i)
    plt.scatter(X[idx, 0], X[idx, 1], c=color, label=i,
                cmap=plt.cm.RdYlBu, edgecolor='black', s=15)


plt.legend(loc='lower right', borderpad=0, handletextpad=0)
plt.axis("tight")


plt.show()

"""###Regression

####Part 1
"""

df = pd.read_csv("/content/drive/MyDrive/PRML/Lab-02/task.csv")
df.dropna()
df.set_index(np.arange(df.shape[0]))
df

X=df["X"].to_numpy()
Y=df["Y"].to_numpy()
X_train,X_test,Y_train,Y_test = tts(X,Y,test_size=0.2,shuffle=True)
print(X_train.shape,X_test.shape)

model = DecisionTreeRegressor(max_depth=2).fit(X_train.reshape(-1,1),Y_train)
X_grid=np.arange(min(X_test),max(X_test),0.01)
X_grid=X_grid.reshape(len(X_grid),1)
Y_pred=model.predict(X_grid)
print(model.score(X_test.reshape(-1,1),Y_test))

plt.plot(X_grid,Y_pred,color="b")
plt.scatter(X_test,Y_test,color="r")
plt.show()

model = DecisionTreeRegressor(max_depth=3).fit(X_train.reshape(-1,1),Y_train)
X_grid=np.arange(min(X_test),max(X_test),0.01)
X_grid=X_grid.reshape(len(X_grid),1)
Y_pred=model.predict(X_grid)
print(model.score(X_test.reshape(-1,1),Y_test))
plt.plot(X_grid,Y_pred,color="b")
plt.scatter(X_test,Y_test,color="r")
plt.show()

"""####Part 2

Regression model with min_samples_leaf = 0
"""

#note min_samples_leaf should be either in (0,0.5] or greater than 1. It was showing error when I am feeding it with zero
model = DecisionTreeRegressor(min_samples_leaf=1).fit(X_train.reshape(-1,1),Y_train)
X_grid=np.arange(min(X_test),max(X_test),0.01)
X_grid=X_grid.reshape(len(X_grid),1)
Y_pred=model.predict(X_grid)
print(model.score(X_test.reshape(-1,1),Y_test))

plt.plot(X_grid,Y_pred,color="b")
plt.scatter(X_test,Y_test,color="r")
plt.show()

"""regression model with min_samples_leaf = 10"""

model = DecisionTreeRegressor(min_samples_leaf=10).fit(X_train.reshape(-1,1),Y_train)
print(model.score(X_test.reshape(-1,1),Y_test))
X_grid=np.arange(min(X_test),max(X_test),0.01)
X_grid=X_grid.reshape(len(X_grid),1)
Y_pred=model.predict(X_grid)

plt.plot(X_grid,Y_pred,color="b")
plt.scatter(X_test,Y_test,color="r")
plt.show()

"""##Problem 03

###Importing Libraries
"""

pip install palmerpenguins

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from palmerpenguins import load_penguins
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split as tts

"""###Part 01

"""

df=load_penguins()
df

df.dropna(inplace=True)
df=df.set_index(np.arange(df.shape[0]))
df.columns

dataheaders = [ 'island', 'sex', 'year']

for header in dataheaders:
  
  sns.catplot(x=header,hue='species',kind='count',data=df)
  plt.show()

dataheaders = [ 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
plt.rcParams['figure.figsize'] = [12, 5]
names=['Adelie','Gentoo','Chinstrap']
# dataheaders=["Area","Perimeter","Compactness","Length of kernel","Width of kernel","Asymmetry coefficient","Length of kernel groove"]
for dataheader,i in zip(dataheaders,range(len(dataheaders))):
  x1 = list(df[df['species'] == names[0]][dataheader])
  x2 = list(df[df['species'] == names[1]][dataheader])
  x3 = list(df[df['species'] == names[2]][dataheader])
  plt.subplot(2,2,i+1)
  
  colors=['blue', 'green', 'orange']
  # names=[1,2,3]

  
  n,bins,patches=plt.hist([x1, x2, x3], color=colors,bins=7, label=names, density=False)

  
  
  
  plt.xlabel(dataheaders[i])
  plt.ylabel("count")
plt.legend(bbox_to_anchor=(1, 0.5))
plt.show()
print('\n\n')

lab_enc = LabelEncoder()
for header in ['island','sex']:
  df[header]=lab_enc.fit_transform(df[header])
df

X=df.drop('species',axis=1)
y=df['species']
X_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,shuffle=True)
X_train.index=np.arange(X_train.shape[0])
y_train.index=np.arange(y_train.shape[0])
print(X_train,)

"""###Part 2"""

def probability(y,classes):
  probs=[]
  for class_name in classes:
    cnt=len(np.where(y==class_name)[0])
    probs.append(cnt/len(y))
  return probs

# def elog(p):
#   if(p==0):
#     return 0
#   else:
#     out=(-1)*p*(math.log(p,2))
#     return out


# def temp_entropy(X,y,feature):
#   total_enp = 0
#   classes=np.unique(y)
#   feature_values = np.unique(X[feature])
#   for feature_value in feature_values:
#     temp_y=y[np.where(X[feature]==feature_value)[0]]
#     temp_prob=probability(temp_y,classes)
#     temp_en=0
#     for p in temp_prob:
      
#       temp_en+=elog(p)
#       # print(elog(p))
#     temp_en*=(len(np.where(X[feature]==feature_value)[0])/len(X[feature]))
#     total_enp+=temp_en
#   return total_enp
  
def entropy(y):
    class_prob=probability(y,np.unique(y))
    enp = 0
    for p in class_prob:
        enp -= p * np.log2(p)
    return enp
    
    
    
        
    

# print(my_entropy(y[np.where(X['sex'])]))
def entropy_of_feature(X,y):
  feature_values=np.unique(X)
  enp=0
  for feature_value in feature_values:
    temp_enp=entropy(y[np.where(X==feature_value)[0]])
    temp_enp*=len(np.where(X==feature_value)[0])/len(X)
    enp+=temp_enp
  return enp




print(entropy_of_feature(X['sex'].to_numpy(),y.to_numpy()))

"""###Part 3"""

def cont_to_cat(X, y):
    
    classes = np.unique(y)
    num_classes = len(classes)

    best_gain = 0
    best_split_value = 0
    parent_enp=entropy(y)
    for split_value in np.unique(X):
      left_y=y[np.where(X<=split_value)[0]]
      right_y=y[np.where(X>split_value)[0]]
      if(len(left_y)!=0 and len(right_y)!=0):
        enp_left=(len(left_y)/len(y))*entropy(left_y)
        enp_right=(len(right_y)/len(y))*entropy(right_y)
        temp_gain=parent_enp-enp_left-enp_right
        if(temp_gain>best_gain):
          best_gain=temp_gain
          best_split_value=split_value
    new_X=X.copy()
    for i in range(len(new_X)):
      new_X[i]=int(X[i]>best_split_value)
    return new_X.astype(int)

dataheaders = [ 'bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']
for header in dataheaders:
  X[header]=cont_to_cat(X[header],y)
print(X,y)

X_train,X_test,y_train,y_test=tts(X,y,test_size=0.3,shuffle=True)

"""###Part 4

"""

def find_best_split(X,y,available_feat):
  best_feature=available_feat[0]
  parent_enp=entropy(y)
  if(parent_enp==0):
    return None,0
  best_gain=0
  for feature in available_feat:
    temp_gain=parent_enp-entropy_of_feature(X[:,feature],y)
    if(best_gain<temp_gain):
      best_feature=feature
      best_gain=temp_gain
  return best_feature,best_gain

def most_probable_class(y):
  classes=np.unique(y)
  
  class_prob=probability(y,classes)
  
  return classes[class_prob.index(max(class_prob))]


class Node:
  def __init__(self,query=[],isLeafNode=0,value=None,childs=[],feature=None):
    self.query=query
    self.isLeafNode=isLeafNode
    self.value=value
    self.childs=childs
    self.feature=feature
    
  
  

class Decision_tree_classifier_scratch:
  def __init__(self,max_depth=100):
    self.max_depth=max_depth
    self.root=None

  

  def __grow_tree(self,X,y,depth,available_feat):

    if(len(available_feat)==0):
      return Node(isLeafNode=1,value=most_probable_class(y))
    best_feat,best_gain = find_best_split(X,y,available_feat)
    # print(best_feat,best_gain)

    
    if(best_gain!=0 and depth<self.max_depth ):
      
      available_feat.remove(best_feat)
       
      new_X=X[:,best_feat]
      feature_values=self.all_feature_values[best_feat]
      # print('feature values',feature_values)
      childs=[]
      for feature_value in feature_values:
        idx=np.where(new_X==feature_value)[0]
        if(len(idx)==0):
          # print('leaf Node',most_probable_class(y))
          childs.append(Node(isLeafNode=1,value=most_probable_class(y)))
        else:

        # print('idx',idx)
          childs.append(self.__grow_tree(X[idx,:],y[idx],depth+1,available_feat))
      
      return Node(query=feature_values,childs=childs,feature=best_feat)


    else:
      # print('leaf Node',most_probable_class(y))
      return Node(isLeafNode=1,value=most_probable_class(y))




      


      

  def train(self,X,y):
    self.no_features=X.shape[1]
    self.all_feature_values=[np.unique(X[:,feature]) for feature in range(X.shape[1])]
    # print(self.all_feature_values)

    self.root=self.__grow_tree(X,y,0,list(range(X.shape[1])))

  def predict(self,x,node=None):
    if(node==None):
      node=self.root
    if(node.isLeafNode):
      return node.value
    to_find=x[node.feature]
    child_no=np.where(node.query==to_find)[0][0]
    
    return self.predict(x,node.childs[child_no])
    
  def test(self,X):
    
    predictions=[]
    for i in range(X.shape[0]):
      pred=self.predict(X[i])
      
      predictions.append(pred)
    return predictions

  def accuracy(self,X,y):
    pred=self.test(X,y)
    classes=np.unique(y)
    class_acc=[]
    total_acc=0
    for class_name in classes:
      idx=np.where(y==class_name)[0]
      cnt=0
      for index in idx:
        if(pred[index]==class_name):
          cnt+=1
          total_acc+=1
      class_acc.append(cnt/len(idx))
    total_acc=total_acc/len(y)
    return total_acc,class_acc

  
  


clf=Decision_tree_classifier_scratch(max_depth=2)
clf.train(X_train.to_numpy(),y_train.to_numpy())
# print((X_train))

print(clf.test(X_test.to_numpy()))
print(clf.accuracy(X_test.to_numpy(),y_test.to_numpy()))
# clf.predict([2,	0,	1,	0,	0,	1,	2007])
# print(find_best_split(X.to_numpy(),y.to_numpy(),range(X.shape[1])))